{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import collections\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "import zipfile\n",
    "\n",
    "import numpy as np\n",
    "from six.moves import urllib\n",
    "from six.moves import xrange  # pylint: disable=redefined-builtin\n",
    "import tensorflow as tf\n",
    "\n",
    "import re\n",
    "import collections\n",
    "import functools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000 :  0.0043\n",
      "20000 : 기자회견장에서 화보 찍는 체조요정들 0.0071\n",
      "30000 : 스토리카드 그 사람이 사랑하는 방법 0.0128\n",
      "40000 : 최수종 갓스트라 스브스뉴스 스토리카드  0.0071\n",
      "50000 : 커피도 체온을 떨어뜨린다  동영상  0.0073\n",
      "총 라인 수 :  51850\n",
      "타이틀 수 :  51826\n",
      "CTR 수 :  51826\n",
      "에러 수 :  24\n",
      "['초강력 워터프루프', '셔츠 초간단 연출법', '센스 넘치는 여행 소품들', '부위별 셀프 운동법', '']\n",
      "[[0.0211], [0.0144], [0.0091], [0.0209], [0.0072]]\n"
     ]
    }
   ],
   "source": [
    "#file load\n",
    "\n",
    "f = open('data/content_list.csv','r')\n",
    "line_counter = 0\n",
    "unsupport_title_counter = 0\n",
    "title_arr = []\n",
    "ctr_arr = []\n",
    "\n",
    "ex_hangul = re.compile('[^ ㄱ-ㅣ가-힣]+') # 한글과 띄어쓰기를 제외한 모든 글자\n",
    "def map_hangul(str):\n",
    "    str = str.replace('_',' ')\n",
    "    str = ex_hangul.sub('',str)\n",
    "    str = str.replace('  ',' ')\n",
    "    return str\n",
    "\n",
    "while True:    \n",
    "    line = f.readline()\n",
    "    if not line: break\n",
    "    line_counter += 1\n",
    "    \n",
    "    line = line.replace('\"\"',' ')\n",
    "    content_info = line.split('\"')\n",
    "    \n",
    "    try:\n",
    "        title = map_hangul(content_info[1])\n",
    "        title = ' '.join(title.split(' ')[2:])\n",
    "        ctr = content_info[2].split(',')[1]\n",
    "        title_arr.append(title)\n",
    "        ctr_arr.append([float(ctr)])\n",
    "        if line_counter % 10000 == 0:\n",
    "            print(line_counter, \":\", title, ctr)\n",
    "    except:\n",
    "        unsupport_title_counter += 1\n",
    "        \n",
    "print('총 라인 수 : ', line_counter)\n",
    "print('타이틀 수 : ', len(title_arr))\n",
    "print('CTR 수 : ', len(ctr_arr))\n",
    "print('에러 수 : ',unsupport_title_counter)\n",
    "    \n",
    "print(title_arr[:5])\n",
    "print(ctr_arr[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "281748\n",
      "['초강력', '워터프루프', '셔츠', '초간단', '연출법', '센스', '넘치는', '여행', '소품들', '부위별']\n"
     ]
    }
   ],
   "source": [
    "#dictionary를 만들기위해 단어만 주욱 모은다\n",
    "#앞의 두 단어 제거 \n",
    "words = functools.reduce(lambda x,y: x+' '+y, title_arr).split(' ')\n",
    "print(len(words))\n",
    "print(words[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "[['UNK', -1], ('', 16738), ('스토리카드', 6442), ('오후', 6333), ('오전', 5251), ('저녁', 5074), ('동영상', 2090), ('남자', 1157), ('중앙일보', 1125), ('여자', 1022), ('한줄', 883), ('가지', 677), ('이유', 671), ('한국일보', 664), ('왜', 612), ('내', 581), ('전자신문', 572), ('수', 557), ('년', 555), ('동아일보', 548)]\n",
      "50000\n",
      "Most common words (+UNK) [['UNK', 17086], ('', 16738), ('스토리카드', 6442), ('오후', 6333), ('오전', 5251)]\n",
      "Sample Data [6870, 13411, 1052, 513, 2118, 2990, 1723, 59, 17088, 15967]\n",
      "Reverse dict ['초강력', '워터프루프', '셔츠', '초간단', '연출법', '센스', '넘치는', '여행', '소품들', '부위별']\n",
      "51826\n"
     ]
    }
   ],
   "source": [
    "vocabulary_size = 50000\n",
    "\n",
    "count = [['UNK', -1]] #unknown\n",
    "count.extend(collections.Counter(words).most_common(vocabulary_size - 1))\n",
    "print(len(count))\n",
    "print(count[:20])\n",
    "\n",
    "#dictionary 정의 & feeding\n",
    "dictionary = dict()\n",
    "for word, _ in count:\n",
    "    dictionary[word] = len(dictionary) #여기서 word는 count를 돌린거라서 유니크함\n",
    "print(len(dictionary))\n",
    "\n",
    "data = list()\n",
    "unk_count = 0\n",
    "\n",
    "for word in words:\n",
    "    if word in dictionary:\n",
    "        index = dictionary[word]\n",
    "    else:\n",
    "        index = 0  # dictionary['UNK']\n",
    "        unk_count += 1\n",
    "    data.append(index)\n",
    "count[0][1] = unk_count\n",
    "    \n",
    "#title_arr 를 index값으로 title_vector_arr로 변환\n",
    "title_vector_arr = []\n",
    "for title in title_arr:\n",
    "    title_vector = []\n",
    "    word_arr = title.split(' ')\n",
    "    for word in word_arr:\n",
    "        if word in dictionary:\n",
    "            index = dictionary[word]\n",
    "        else:\n",
    "            index = 0\n",
    "        title_vector.append(index)\n",
    "    title_vector_arr.append(title_vector)\n",
    "    \n",
    "reverse_dictionary = dict(zip(dictionary.values(), dictionary.keys()))\n",
    "\n",
    "# del words  # Hint to reduce memory.\n",
    "print('Most common words (+UNK)', count[:5])\n",
    "print('Sample Data', data[:10])\n",
    "print('Reverse dict', [reverse_dictionary[word_index] for word_index in data[:10]])\n",
    "print(len(title_vector_arr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title vector array sample [[6870, 13411, -1, -1, -1, -1, -1, -1, -1, -1], [1052, 513, 2118, -1, -1, -1, -1, -1, -1, -1], [2990, 1723, 59, 17088, -1, -1, -1, -1, -1, -1], [15967, 602, 711, -1, -1, -1, -1, -1, -1, -1], [1, -1, -1, -1, -1, -1, -1, -1, -1, -1], [7637, 12659, 4, -1, -1, -1, -1, -1, -1, -1], [3194, 30085, 3, -1, -1, -1, -1, -1, -1, -1], [36096, 16196, 40582, 9323, 45744, 0, 10279, 5, -1, -1], [2815, 0, 18422, 11894, 947, 8132, 156, 1007, 5, -1], [4482, 43, 1635, 21707, 459, 477, 5, -1, -1, -1]]\n"
     ]
    }
   ],
   "source": [
    "# max_length = 0\n",
    "# for title_vector in title_vector_arr:\n",
    "#     if max_length < len(title_vector):\n",
    "#         max_length = len(title_vector)\n",
    "# print(max_length)\n",
    "\n",
    "max_length = 10\n",
    "#vector 사이즈를 정의하고, 부족한 부분은 empty_index로 채워서 사이즈를 맞춘다\n",
    "empty_index = -1\n",
    "title_vector_size = max_length\n",
    "index = 0\n",
    "for title_vector in title_vector_arr:\n",
    "    while(len(title_vector) < title_vector_size):\n",
    "        title_vector.append(empty_index)\n",
    "    title_vector_arr[index] = title_vector[0:max_length]\n",
    "    index +=1\n",
    "print('Title vector array sample', title_vector_arr[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13411 워터프루프 -> 1052 셔츠\n",
      "13411 워터프루프 -> 6870 초강력\n",
      "1052 셔츠 -> 513 초간단\n",
      "1052 셔츠 -> 13411 워터프루프\n",
      "513 초간단 -> 1052 셔츠\n",
      "513 초간단 -> 2118 연출법\n",
      "2118 연출법 -> 2990 센스\n",
      "2118 연출법 -> 513 초간단\n"
     ]
    }
   ],
   "source": [
    "data_index = 0\n",
    "\n",
    "# Step 3: Function to generate a training batch for the skip-gram model.\n",
    "def generate_batch(batch_size, num_skips, skip_window):\n",
    "  global data_index\n",
    "  assert batch_size % num_skips == 0\n",
    "  assert num_skips <= 2 * skip_window\n",
    "  batch = np.ndarray(shape=(batch_size), dtype=np.int32)\n",
    "  labels = np.ndarray(shape=(batch_size, 1), dtype=np.int32)\n",
    "  span = 2 * skip_window + 1  # [ skip_window target skip_window ]\n",
    "  buffer = collections.deque(maxlen=span)\n",
    "  for _ in range(span):\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "  for i in range(batch_size // num_skips):\n",
    "    target = skip_window  # target label at the center of the buffer\n",
    "    targets_to_avoid = [skip_window]\n",
    "    for j in range(num_skips):\n",
    "      while target in targets_to_avoid:\n",
    "        target = random.randint(0, span - 1)\n",
    "      targets_to_avoid.append(target)\n",
    "      batch[i * num_skips + j] = buffer[skip_window]\n",
    "      labels[i * num_skips + j, 0] = buffer[target]\n",
    "    buffer.append(data[data_index])\n",
    "    data_index = (data_index + 1) % len(data)\n",
    "  # Backtrack a little bit to avoid skipping words in the end of a batch\n",
    "  data_index = (data_index + len(data) - span) % len(data)\n",
    "  return batch, labels\n",
    "\n",
    "batch, labels = generate_batch(batch_size=8, num_skips=2, skip_window=1)\n",
    "for i in range(8):\n",
    "  print(batch[i], reverse_dictionary[batch[i]],\n",
    "        '->', labels[i, 0], reverse_dictionary[labels[i, 0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def xavier_init(n_inputs, n_outputs):\n",
    "    init_range = tf.sqrt(6.0/(n_inputs+n_outputs))\n",
    "    return tf.random_uniform_initializer(-init_range, init_range)\n",
    "\n",
    "grade_size = len(grade)\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(51826, title_vector_size), name='title_vector_arr')\n",
    "Y = tf.placeholder(tf.float32, shape=(51826, grade_size), name='ctr_grade_arr')\n",
    "\n",
    "neuron_width = 200\n",
    "W1 = tf.get_variable(shape=[title_vector_size, neuron_width], initializer=xavier_init(title_vector_size, neuron_width), name='hidden1')\n",
    "W2 = tf.get_variable(shape=[neuron_width, neuron_width], initializer=xavier_init(neuron_width, neuron_width), name='hidden2')\n",
    "W3 = tf.get_variable(shape=[neuron_width, neuron_width], initializer=xavier_init(neuron_width, neuron_width), name='hidden3')\n",
    "W4 = tf.get_variable(shape=[neuron_width, grade_size], initializer=xavier_init(neuron_width, grade_size), name='hidden4')\n",
    "\n",
    "b1 = tf.Variable(tf.zeros([neuron_width]), name='bias1')\n",
    "b2 = tf.Variable(tf.zeros([neuron_width]), name='bias2')\n",
    "b3 = tf.Variable(tf.zeros([neuron_width]), name='bias3')\n",
    "b4 = tf.Variable(tf.zeros([grade_size]), name='bias4')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embed :  Tensor(\"embedding_lookup:0\", shape=(128, 128), dtype=float32, device=/device:CPU:0)\n"
     ]
    }
   ],
   "source": [
    "# Step 4: Build and train a skip-gram model.\n",
    "\n",
    "batch_size = 128\n",
    "embedding_size = 128  # Dimension of the embedding vector.\n",
    "skip_window = 1       # How many words to consider left and right.\n",
    "num_skips = 2         # How many times to reuse an input to generate a label.\n",
    "\n",
    "# We pick a random validation set to sample nearest neighbors. Here we limit the\n",
    "# validation samples to the words that have a low numeric ID, which by\n",
    "# construction are also the most frequent.\n",
    "valid_size = 16     # Random set of words to evaluate similarity on.\n",
    "valid_window = 100  # Only pick dev samples in the head of the distribution.\n",
    "valid_examples = np.random.choice(valid_window, valid_size, replace=False)\n",
    "num_sampled = 64    # Number of negative examples to sample.\n",
    "\n",
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data.\n",
    "  train_inputs = tf.placeholder(tf.int32, shape=[batch_size])\n",
    "  train_labels = tf.placeholder(tf.int32, shape=[batch_size, 1])\n",
    "  valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
    "\n",
    "  # Ops and variables pinned to the CPU because of missing GPU implementation\n",
    "  with tf.device('/cpu:0'):\n",
    "    # Look up embeddings for inputs.\n",
    "    embeddings = tf.Variable(\n",
    "        tf.random_uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
    "    embed = tf.nn.embedding_lookup(embeddings, train_inputs)  #train_inputs에 있는 id로 embeddings를 lookup 한 값을 가지고 있음\n",
    "    print(\"embed : \",embed)\n",
    "\n",
    "    # Construct the variables for the NCE loss\n",
    "    nce_weights = tf.Variable(\n",
    "        tf.truncated_normal([vocabulary_size, embedding_size],\n",
    "                            stddev=1.0 / math.sqrt(embedding_size)))\n",
    "    nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
    "\n",
    "  # Compute the average NCE loss for the batch.\n",
    "  # tf.nce_loss automatically draws a new sample of the negative labels each\n",
    "  # time we evaluate the loss.\n",
    "  loss = tf.reduce_mean(\n",
    "      tf.nn.nce_loss(weights=nce_weights,\n",
    "                     biases=nce_biases,\n",
    "                     labels=train_labels,\n",
    "                     inputs=embed,\n",
    "                     num_sampled=num_sampled,\n",
    "                     num_classes=vocabulary_size))\n",
    "\n",
    "  # Construct the SGD optimizer using a learning rate of 1.0.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(1.0).minimize(loss)\n",
    "\n",
    "  # Compute the cosine similarity between minibatch examples and all embeddings.\n",
    "  norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keep_dims=True))\n",
    "  normalized_embeddings = embeddings / norm\n",
    "  valid_embeddings = tf.nn.embedding_lookup(\n",
    "      normalized_embeddings, valid_dataset)\n",
    "  similarity = tf.matmul(\n",
    "      valid_embeddings, normalized_embeddings, transpose_b=True)\n",
    "\n",
    "  # Add variable initializer.\n",
    "  init = tf.global_variables_initializer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Average loss at step  0 :  273.780822754\n",
      "Nearest to 여성: 남녀싸움, 전기차들, 일부러, 소생시킬, 좋아하시나요, 더도, 고치려고, 세진다,\n",
      "Nearest to 저녁: 패배원인, 쫀쫀한, 충동범죄, 칸으로, 합격에, 노른자를, 황재균유희관이, 애도,\n",
      "Nearest to 법: 날씬했다, 그날의, 김종국에게, 멘탈에, 무장한, 기겁, 껌의, 황제의,\n",
      "Nearest to 여행: 운동해야, 털리더라, 갈색, 곳은, 김연지, 동료와, 신입생들에, 후속,\n",
      "Nearest to 스타투데이: 디오니소스, 끈끈했지만, 손마사지, 갈때, 대사하지마, 불타는, 보약, 여섯번째,\n",
      "Nearest to 쉐어하우스: 바르니, 끝없다, 반토막, 파우더, 인하도, 체면에, 대하나, 전당된,\n",
      "Nearest to 한국경제: 어민, 다리를, 맹수들앞에서, 아내와, 건조한, 집값, 울리다, 미궁,\n",
      "Nearest to 중: 끝판대장, 순간포착, 시리즈, 말한, 김원준, 아이큐는, 청바지에, 인정받으려,\n",
      "Nearest to 때: 물렀거라, 반응은, 추천해요, 도심, 마성의, 벗긴, 운용, 이하늬,\n",
      "Nearest to 무슨: 대중이, 허리통증, 유망주, 닮아도, 슈퍼마리오, 애완견의, 여성호르몬의, 익혀,\n",
      "Nearest to 다이어트: 베프반짝반짝, 상경, 회사를, 스크린에서, 예상, 잠을, 갇혔을, 사죄한,\n",
      "Nearest to 첫: 걸어봤니, 환영교사는, 여자아이의, 하이파이브, 초로, 연예기자또, 약속이다, 베란다까지,\n",
      "Nearest to 뉴스에이드: 낮이, 욕했어, 이순신, 소변기, 향수는, 최강의, 앞입니다, 노는,\n",
      "Nearest to 만원: 마스크맨들, 확률은, 스캔들의, 잡으려, 고기도, 돌파구, 서러운데씁쓸한, 증상도,\n",
      "Nearest to 하는: 파파라치포착, 싶고, 싶었어, 암은, 당신이거절당하는이유, 나이드는법, 현희씨의, 슬픈,\n",
      "Nearest to 전: 일곱, 예능꾼들, 비슷한, 몸싸움해, 낳은, 아침부터, 위생, 표도르,\n",
      "Average loss at step  2000 :  129.547622999\n",
      "Average loss at step  4000 :  63.502275351\n",
      "Average loss at step  6000 :  39.1683434763\n",
      "Average loss at step  8000 :  26.0887414126\n",
      "Average loss at step  10000 :  18.5882169082\n",
      "Nearest to 여성: 일부러, 가로수길, 소생시킬, 남자는, 이태임예원, 가득, 남녀싸움, 피곤한,\n",
      "Nearest to 저녁: 오후, 오전, UNK, , 우리나라, 확인, 추석인데, 들이대다,\n",
      "Nearest to 법: 날씬했다, 껌의, 그날의, 황제의, 김종국에게, 수아레스, 남녀, 멘탈에,\n",
      "Nearest to 여행: 곳은, 운동해야, 털리더라, 갈색, 매출, 동료와, 아무도, 오나,\n",
      "Nearest to 스타투데이: 저질렀길래, 불타는, 보약, 끝난, 물에, 모른다, 대해, 시계의,\n",
      "Nearest to 쉐어하우스: 모바일, 파우더, 정도, 모터그래프, 신인, 마디, 아침, 단발머리,\n",
      "Nearest to 한국경제: 다리를, 집값, 건조한, 아내와, 만들어, 더트래블러, 어민, 짓까지,\n",
      "Nearest to 중: 순간포착, 시리즈, 정신, 끝판대장, 눈에, 예상, 보디, 말한,\n",
      "Nearest to 때: 반응은, 물렀거라, 도심, 연기로, 이하늬, 힐링이, 추천해요, 자동차가,\n",
      "Nearest to 무슨: 유망주, 문장, 대중이, 금, 스캔들, 하나면, 슈퍼마리오, 난다,\n",
      "Nearest to 다이어트: 예상, 쓰면, 회사를, 외로운, 잠을, 산이, 끼, 먹어봤어,\n",
      "Nearest to 첫: 싶어요, 하이파이브, 연말을, 만능, 이리, 여자아이의, 강정호, 눈에,\n",
      "Nearest to 뉴스에이드: 최강의, 이순신, 향수는, 벤츠, 지갑, 손으로, 팔리는, 낮이,\n",
      "Nearest to 만원: 마스크맨들, 확률은, 씨스타, 달라지는, 파괴적인, 빵빵, 과연, 스캔들의,\n",
      "Nearest to 하는: 끔찍한, 세단, 먼저, 암은, 일부, 생각보다, 가짜, 여자친구가,\n",
      "Nearest to 전: 비슷한, 일곱, 포르쉐, 예능꾼들, 위생, 먼치킨, 자신이, 예뻐,\n",
      "Average loss at step  12000 :  13.1247431777\n",
      "Average loss at step  14000 :  10.0359026787\n",
      "Average loss at step  16000 :  8.40812376523\n",
      "Average loss at step  18000 :  6.98219213521\n",
      "Average loss at step  20000 :  6.32584422183\n",
      "Nearest to 여성: 남녀싸움, 이태임예원, 소생시킬, 한심한, 전기차들, 가로수길, 가득, 폐업,\n",
      "Nearest to 저녁: 오후, 오전, 확인, , 우리나라, 추석인데, 학기, 여경,\n",
      "Nearest to 법: 날씬했다, 방법, 황제의, 멘탈에, 선수들이, 피순대, UNK, 껌의,\n",
      "Nearest to 여행: 운동해야, 곳은, 갈색, 털리더라, 동료와, 랭귀지, 오나, 아무도,\n",
      "Nearest to 스타투데이: 저질렀길래, 불타는, 김이나, 보약, 대사하지마, 갈때, 물에, 콜로라도,\n",
      "Nearest to 쉐어하우스: 파우더, 연비, 대인데, 단발머리, 모바일, 끝없다, 청춘의, 마디,\n",
      "Nearest to 한국경제: 다리를, 어민, 건조한, 집값, 맹수들앞에서, 타면, 부었는데, 값이,\n",
      "Nearest to 중: 순간포착, 끝판대장, 시리즈, 예상, 정신, 용사는, 청바지에, 신인의,\n",
      "Nearest to 때: 물렀거라, 반응은, 자동차가, 운용, 추천해요, 애플이, 연기로, 이하늬,\n",
      "Nearest to 무슨: 유망주, 허리통증, 문장, 대중이, 닮아도, 런치, 수영장에서, 자유인가,\n",
      "Nearest to 다이어트: 회사를, 상경, 예상, 블랙화이트, 잠을, 쓰면, UNK, 무선을,\n",
      "Nearest to 첫: 하이파이브, 베란다까지, 콘카츠, 연예기자또, 여자아이의, 싶어요, 뭐하는, 선수가,\n",
      "Nearest to 뉴스에이드: 향수는, 낮이, 최강의, 아이에요, 이순신, 지갑, 무명, 팔리는,\n",
      "Nearest to 만원: 마스크맨들, 씨스타, 확률은, 달라지는, 꽃들의, 고기도, 파괴적인, 증상도,\n",
      "Nearest to 하는: 세단, 일부, 나이드는법, 파파라치포착, 여자친구가, 근거, 끔찍한, 첫째,\n",
      "Nearest to 전: 예능꾼들, 먼치킨, 비슷한, 일곱, 위생, 포르쉐, 아침부터, 표현하기,\n",
      "Average loss at step  22000 :  5.47209113348\n",
      "Average loss at step  24000 :  5.34398321581\n",
      "Average loss at step  26000 :  4.94809855425\n",
      "Average loss at step  28000 :  4.74654620898\n",
      "Average loss at step  30000 :  4.58430775046\n",
      "Nearest to 여성: 남녀싸움, 이태임예원, 한심한, 소생시킬, 전기차들, 극과극, 가로수길, 가득,\n",
      "Nearest to 저녁: 오후, 오전, 추석인데, 판치는, 퇴소식서, 가는길, 확인, 우리나라,\n",
      "Nearest to 법: 방법, 날씬했다, 선수들이, 피순대, 황제의, 남성성, 멘탈에, 작가들,\n",
      "Nearest to 여행: 운동해야, 곳은, 갈색, 털리더라, 랭귀지, 살찌게하는, 후속, 동료와,\n",
      "Nearest to 스타투데이: 저질렀길래, 김이나, 대사하지마, 불타는, 갈때, 갱년기, 영입까지, 콜로라도,\n",
      "Nearest to 쉐어하우스: 파우더, 대인데, 연비, 끝없다, 인하도, 에듀케이션, 남자스타들의, 단발머리,\n",
      "Nearest to 한국경제: 다리를, 어민, 집값, 건조한, 이가탄, 더트래블러, 타면, 비밀로,\n",
      "Nearest to 중: 순간포착, 끝판대장, 용사는, 아이큐는, 신인의, 예상, 청바지에, 정신,\n",
      "Nearest to 때: 물렀거라, 운용, 반응은, 자동차가, 추천해요, 애플이, 이하늬, 전북,\n",
      "Nearest to 무슨: 허리통증, 유망주, 문장, 대중이, 닮아도, 장담은, 김소영, 런치,\n",
      "Nearest to 다이어트: 회사를, 상경, 잠을, 블랙화이트, 아이라면막막하네, 무선을, 스케줄은, 검색왕,\n",
      "Nearest to 첫: 베란다까지, 연예기자또, 콘카츠, 걸어봤니, 하이파이브, 유럽서, 선수가, 오광록,\n",
      "Nearest to 뉴스에이드: 향수는, 낮이, 아이에요, 최강의, 무명, 이순신, 제조업, 없나요,\n",
      "Nearest to 만원: 마스크맨들, 씨스타, 확률은, 꽃들의, 고기도, 달라지는, 카이맨, 파괴적인,\n",
      "Nearest to 하는: 일부, 세단, 나이드는법, 파파라치포착, 여자친구가, 소송중인, 첫째, 근거,\n",
      "Nearest to 전: 먼치킨, 예능꾼들, 비슷한, 위생, 일곱, 포르쉐, 아침부터, 표현하기,\n",
      "Average loss at step  32000 :  4.39098137951\n",
      "Average loss at step  34000 :  4.35224686587\n",
      "Average loss at step  36000 :  4.06788125932\n",
      "Average loss at step  38000 :  4.14381739163\n",
      "Average loss at step  40000 :  3.94999750197\n",
      "Nearest to 여성: 남녀싸움, 이태임예원, 한심한, 소생시킬, 극과극, 전기차들, 가로수길, 집밥을,\n",
      "Nearest to 저녁: 오후, 퇴소식서, 가는길, 판치는, 추석인데, 김포에, 오전, 그립습니다,\n",
      "Nearest to 법: 방법, 날씬했다, 피순대, 비행자동차, 작가들, 남성성, 선수들이, 글루건으로,\n",
      "Nearest to 여행: 운동해야, 곳은, 갈색, 살찌게하는, 후속, 랭귀지, 탄생하게, 동료와,\n",
      "Nearest to 스타투데이: 김이나, 저질렀길래, 대사하지마, 영입까지, 갱년기, 갈때, 콜로라도, 불타는,\n",
      "Nearest to 쉐어하우스: 파우더, 남자스타들의, 대인데, 인하도, 최소사망은, 연비, 에듀케이션, 끝없다,\n",
      "Nearest to 한국경제: 다리를, 어민, 집값, 이가탄, 건조한, 타면, 맹수들앞에서, 더트래블러,\n",
      "Nearest to 중: 순간포착, 끝판대장, 용사는, 아이큐는, 신인의, UNK, 정신, 특사로,\n",
      "Nearest to 때: 물렀거라, 운용, 자동차가, 반응은, 추천해요, 이하늬, 다른세상, 전북,\n",
      "Nearest to 무슨: 허리통증, 유망주, 닮아도, 대중이, 문장, 김소영, 장담은, 런치,\n",
      "Nearest to 다이어트: 회사를, 상경, 아이라면막막하네, 잠을, 무선을, 검색왕, 스케줄은, 국제커플,\n",
      "Nearest to 첫: 베란다까지, 걸어봤니, 연예기자또, 콘카츠, 유럽서, 행복한가, 아빠만의, 이리,\n",
      "Nearest to 뉴스에이드: 향수는, 낮이, 무명, 아이에요, 최강의, 이순신, 제조업, 없나요,\n",
      "Nearest to 만원: 마스크맨들, 씨스타, 꽃들의, 고기도, 확률은, 카이맨, 달라지는, 블라인드,\n",
      "Nearest to 하는: 일부, 세단, 소송중인, 나이드는법, 첫째, 산타의, 남국에서, 파파라치포착,\n",
      "Nearest to 전: 먼치킨, 예능꾼들, 위생, 비슷한, 일곱, 포르쉐, 아침부터, 표현하기,\n",
      "Average loss at step  42000 :  3.97540137768\n",
      "Average loss at step  44000 :  3.85382416809\n",
      "Average loss at step  46000 :  3.86594201922\n",
      "Average loss at step  48000 :  3.78760310435\n",
      "Average loss at step  50000 :  3.73389529514\n",
      "Nearest to 여성: 남녀싸움, 이태임예원, 한심한, 소생시킬, 가로수길, 극과극, 전기차들, 집밥을,\n",
      "Nearest to 저녁: 퇴소식서, 추석인데, 가는길, 오후, 신이된다, 김포에, 판치는, 알레리아,\n",
      "Nearest to 법: 방법, 날씬했다, 비행자동차, 남성성, 작가들, 글루건으로, 피순대, 선수들이,\n",
      "Nearest to 여행: 운동해야, 곳은, 살찌게하는, 후속, 갈색, 탄생하게, 랭귀지, 김연지,\n",
      "Nearest to 스타투데이: 김이나, 영입까지, 대사하지마, 저질렀길래, 갱년기, 갈때, 콜로라도, 존중합니다,\n",
      "Nearest to 쉐어하우스: 파우더, 남자스타들의, 대인데, 최소사망은, 인하도, 연비, 끝없다, 비둘기가,\n",
      "Nearest to 한국경제: 다리를, 어민, 집값, 이가탄, 건조한, 타면, 맹수들앞에서, 비밀로,\n",
      "Nearest to 중: 순간포착, 끝판대장, 용사는, 아이큐는, 신인의, 특사로, 어쩌지, 유기물까지,\n",
      "Nearest to 때: 물렀거라, 운용, 자동차가, 반응은, 다른세상, 이하늬, 전북, 추천해요,\n",
      "Nearest to 무슨: 허리통증, 유망주, 김소영, 닮아도, 대중이, 문장, 장담은, 런치,\n",
      "Nearest to 다이어트: 회사를, 상경, 아이라면막막하네, 잠을, 스케줄은, 꽈리고추, 검색왕, 국제커플,\n",
      "Nearest to 첫: 베란다까지, 걸어봤니, 유럽서, 행복한가, 연예기자또, 아빠만의, 콘카츠, 바둑중계를,\n",
      "Nearest to 뉴스에이드: 향수는, 낮이, 무명, 아이에요, 최강의, 이순신, 없나요, 개선하는,\n",
      "Nearest to 만원: 마스크맨들, 씨스타, 꽃들의, 고기도, 확률은, 블라인드, 카이맨, 달라지는,\n",
      "Nearest to 하는: 일부, 첫째, 세단, 소송중인, 남국에서, 산타의, 간담이, 나이드는법,\n",
      "Nearest to 전: 먼치킨, 예능꾼들, 위생, 비슷한, 일곱, 포르쉐, 표현하기, 신기술,\n",
      "Average loss at step  52000 :  3.72718978834\n",
      "Average loss at step  54000 :  3.63995964348\n",
      "Average loss at step  56000 :  3.66773110473\n",
      "Average loss at step  58000 :  3.57343673432\n",
      "Average loss at step  60000 :  3.57899060559\n",
      "Nearest to 여성: 남녀싸움, 이태임예원, 한심한, 소생시킬, 가로수길, 극과극, 전기차들, 집밥을,\n",
      "Nearest to 저녁: 추석인데, 퇴소식서, 신이된다, 가는길, 판치는, 알레리아, 그립습니다, 근질근질,\n",
      "Nearest to 법: 방법, 날씬했다, 남성성, 글루건으로, 작가들, 이상민의, 청춘이다, 비행자동차,\n",
      "Nearest to 여행: 운동해야, 곳은, 후속, 살찌게하는, 탄생하게, 갈색, 랭귀지, 김연지,\n",
      "Nearest to 스타투데이: 김이나, 영입까지, 대사하지마, 갱년기, 저질렀길래, 갈때, 콜로라도, 훌쩍,\n",
      "Nearest to 쉐어하우스: 파우더, 남자스타들의, 대인데, 최소사망은, 비둘기가, 인하도, 교실에, 광파링의,\n",
      "Nearest to 한국경제: 어민, 다리를, 집값, 이가탄, 건조한, 타면, 맹수들앞에서, 프랑스판,\n",
      "Nearest to 중: 순간포착, 끝판대장, 용사는, 신인의, 아이큐는, 특사로, 강추위를, 말한,\n",
      "Nearest to 때: 물렀거라, 운용, 반응은, 자동차가, 다른세상, 이하늬, 전북, 추천해요,\n",
      "Nearest to 무슨: 허리통증, 김소영, 닮아도, 유망주, 대중이, 장담은, 문장, 수영장에서,\n",
      "Nearest to 다이어트: 회사를, 상경, 아이라면막막하네, 잠을, 스케줄은, 꽈리고추, 휘두른, 국제커플,\n",
      "Nearest to 첫: 베란다까지, 걸어봤니, 행복한가, 아빠만의, 유럽서, 연예기자또, 콘카츠, 일명,\n",
      "Nearest to 뉴스에이드: 향수는, 무명, 낮이, 아이에요, 이순신, 최강의, 없나요, 도와주냐,\n",
      "Nearest to 만원: 마스크맨들, 씨스타, 꽃들의, 고기도, 확률은, 블라인드, 카이맨, 돌파구,\n",
      "Nearest to 하는: 일부, 첫째, 소송중인, 산타의, 샌디스크코리아, 남국에서, 아무, 피카소처럼,\n",
      "Nearest to 전: 먼치킨, 예능꾼들, 위생, 일곱, 비슷한, 포르쉐, 표현하기, 신기술,\n",
      "Average loss at step  62000 :  3.54146664417\n",
      "Average loss at step  64000 :  3.49873951054\n",
      "Average loss at step  66000 :  3.48559620512\n",
      "Average loss at step  68000 :  3.43666671515\n",
      "Average loss at step  70000 :  3.43784474277\n",
      "Nearest to 여성: 남녀싸움, 이태임예원, 한심한, 소생시킬, 가로수길, 남성, 극과극, 집밥을,\n",
      "Nearest to 저녁: 퇴소식서, 가는길, 추석인데, 그립습니다, 판치는, 진정한리더는직접쓰고, 신이된다, 영국에서,\n",
      "Nearest to 법: 방법, 날씬했다, 글루건으로, 작가들, 남성성, 이상민의, 청춘이다, 비행자동차,\n",
      "Nearest to 여행: 운동해야, 곳은, 탄생하게, 후속, 살찌게하는, 이가탄, 노화에, 김연지,\n",
      "Nearest to 스타투데이: 영입까지, 김이나, 갱년기, 대사하지마, 갈때, 콜로라도, 훌쩍, 합병안,\n",
      "Nearest to 쉐어하우스: 파우더, 남자스타들의, 대인데, 교실에, 인하도, 광파링의, 끝없다, 비둘기가,\n",
      "Nearest to 한국경제: 어민, 집값, 다리를, 이가탄, 타면, 건조한, 맹수들앞에서, 프랑스판,\n",
      "Nearest to 중: 순간포착, 끝판대장, 용사는, 신인의, 특사로, 아이큐는, 말한, 유기물까지,\n",
      "Nearest to 때: 물렀거라, 다른세상, 운용, 자동차가, 이하늬, 전북, 반응은, 이것은,\n",
      "Nearest to 무슨: 허리통증, 김소영, 닮아도, 유망주, 장담은, 대중이, 수영장에서, 애완견의,\n",
      "Nearest to 다이어트: 회사를, 상경, 잠을, 아이라면막막하네, 꽈리고추, 스케줄은, 휘두른, 무선을,\n",
      "Nearest to 첫: 베란다까지, 걸어봤니, 행복한가, 아빠만의, 연예기자또, 유럽서, 일명, 드라기,\n",
      "Nearest to 뉴스에이드: 향수는, 이순신, 무명, 아이에요, 도와주냐, 낮이, 먹는다고, 없나요,\n",
      "Nearest to 만원: 마스크맨들, 씨스타, 꽃들의, 고기도, 확률은, 카이맨, 블라인드, 돌파구,\n",
      "Nearest to 하는: 첫째, 일부, 산타의, 소송중인, 서림, 샌디스크코리아, 간담이, 아무,\n",
      "Nearest to 전: 먼치킨, 예능꾼들, 위생, 일곱, 비슷한, 신기술, 표현하기, 불치병,\n",
      "Average loss at step  72000 :  3.37407258964\n",
      "Average loss at step  74000 :  3.39895851445\n",
      "Average loss at step  76000 :  3.33724844706\n",
      "Average loss at step  78000 :  3.33670986152\n",
      "Average loss at step  80000 :  3.31141450906\n",
      "Nearest to 여성: 남녀싸움, 이태임예원, 한심한, 소생시킬, 가로수길, 남성, 극과극, 공포영화는,\n",
      "Nearest to 저녁: 추석인데, 퇴소식서, 가는길, 김포에, 판치는, 그립습니다, 박태환의, 근질근질,\n",
      "Nearest to 법: 방법, 날씬했다, 글루건으로, 이상민의, 청춘이다, 남성성, 작가들, 런지동작으로,\n",
      "Nearest to 여행: 운동해야, 곳은, 탄생하게, 후속, 살찌게하는, 이가탄, 중고차값은, 베팅,\n",
      "Nearest to 스타투데이: 영입까지, 김이나, 갱년기, 대사하지마, 콜로라도, 훌쩍, 합병안, 갈때,\n",
      "Nearest to 쉐어하우스: 파우더, 남자스타들의, 교실에, 비둘기가, 광파링의, 대인데, 되고도, 인하도,\n",
      "Nearest to 한국경제: 어민, 집값, 다리를, 이가탄, 타면, 건조한, 프랑스판, 맹수들앞에서,\n",
      "Nearest to 중: 끝판대장, 순간포착, 용사는, 특사로, 신인의, 말한, 아이큐는, 강추위를,\n",
      "Nearest to 때: 물렀거라, 다른세상, 운용, 자동차가, 전북, 이하늬, 반응은, 평가절하,\n",
      "Nearest to 무슨: 허리통증, 김소영, 닮아도, 유망주, 대중이, 장담은, 수영장에서, 웃음바다,\n",
      "Nearest to 다이어트: 회사를, 상경, 잠을, 휘두른, 아이라면막막하네, 스케줄은, 꽈리고추, 조금씩,\n",
      "Nearest to 첫: 베란다까지, 걸어봤니, 행복한가, 아빠만의, 드라기, 연예기자또, 유럽서, 학력,\n",
      "Nearest to 뉴스에이드: 향수는, 이순신, 대자, 아이에요, 도와주냐, 무명, 낮이, 먹는다고,\n",
      "Nearest to 만원: 마스크맨들, 씨스타, 꽃들의, 고기도, 확률은, 카이맨, 블라인드, 돌파구,\n",
      "Nearest to 하는: 첫째, 아무, 쭉쭉, 산타의, 샌디스크코리아, 간담이, 일부, 서림,\n",
      "Nearest to 전: 먼치킨, 예능꾼들, 일곱, 위생, 신기술, 비슷한, 표현하기, 불치병,\n",
      "Average loss at step  82000 :  3.26991528285\n",
      "Average loss at step  84000 :  3.30584561342\n",
      "Average loss at step  86000 :  3.22200897777\n",
      "Average loss at step  88000 :  3.26441076732\n",
      "Average loss at step  90000 :  3.17825081515\n",
      "Nearest to 여성: 남녀싸움, 이태임예원, 남성, 가로수길, 소생시킬, 한심한, 극과극, 공포영화는,\n",
      "Nearest to 저녁: 퇴소식서, 추석인데, 가는길, 알레리아, 그립습니다, 신이된다, 근질근질, 국민남친,\n",
      "Nearest to 법: 방법, 글루건으로, 날씬했다, 이상민의, 남성성, 청춘이다, 런지동작으로, 작가들,\n",
      "Nearest to 여행: 운동해야, 곳은, 탄생하게, 살찌게하는, 이가탄, 후속, 중고차값은, 베팅,\n",
      "Nearest to 스타투데이: 영입까지, 김이나, 갱년기, 합병안, 훌쩍, 콜로라도, 대사하지마, 갈때,\n",
      "Nearest to 쉐어하우스: 파우더, 남자스타들의, 교실에, 광파링의, 비둘기가, 인하도, 대인데, 영화배우,\n",
      "Nearest to 한국경제: 어민, 집값, 다리를, 이가탄, 타면, 프랑스판, 건조한, 맹수들앞에서,\n",
      "Nearest to 중: 순간포착, 끝판대장, 용사는, 특사로, 신인의, 강추위를, 말한, 메이비를,\n",
      "Nearest to 때: 물렀거라, 다른세상, 이하늬, 전북, 자동차가, 운용, 평가절하, 오픈했다고,\n",
      "Nearest to 무슨: 허리통증, 김소영, 닮아도, 장담은, 대중이, 유망주, 수영장에서, 웃음바다,\n",
      "Nearest to 다이어트: 회사를, 상경, 잠을, 아이라면막막하네, 스케줄은, 꽈리고추, 게임개발, 돌려서,\n",
      "Nearest to 첫: 베란다까지, 걸어봤니, 행복한가, 아빠만의, 일명, 드라기, 학력, 연예기자또,\n",
      "Nearest to 뉴스에이드: 향수는, 이순신, 대자, 도와주냐, 먹는다고, 없나요, 아이에요, 낮이,\n",
      "Nearest to 만원: 마스크맨들, 꽃들의, 씨스타, 고기도, 확률은, 카이맨, 블라인드, 보이,\n",
      "Nearest to 하는: 아무, 첫째, 쭉쭉, 간담이, 서림, 샌디스크코리아, 산타의, 소송중인,\n",
      "Nearest to 전: 먼치킨, 예능꾼들, 일곱, 위생, 표현하기, 불치병, 비슷한, 신기술,\n",
      "Average loss at step  92000 :  3.22317282677\n",
      "Average loss at step  94000 :  3.14357630467\n",
      "Average loss at step  96000 :  3.17816730571\n",
      "Average loss at step  98000 :  3.13726656771\n",
      "Average loss at step  100000 :  3.12436766136\n",
      "Nearest to 여성: 남녀싸움, 남성, 이태임예원, 가로수길, 소생시킬, 한심한, 라스베이거스에서, 공포영화는,\n",
      "Nearest to 저녁: 추석인데, 퇴소식서, 신이된다, 가는길, 근질근질, 경쟁, 그립습니다, 알레리아,\n",
      "Nearest to 법: 방법, 글루건으로, 이상민의, 청춘이다, 날씬했다, 런지동작으로, 남성성, 비밀얘기,\n",
      "Nearest to 여행: 운동해야, 곳은, 탄생하게, 살찌게하는, 이가탄, 후속, 중고차값은, 베팅,\n",
      "Nearest to 스타투데이: 영입까지, 김이나, 갱년기, 합병안, 훌쩍, 콜로라도, 한국경제, 갈때,\n",
      "Nearest to 쉐어하우스: 파우더, 남자스타들의, 교실에, 광파링의, 김은, 봉쥬르, 코스모폴리탄, 높이는,\n",
      "Nearest to 한국경제: 어민, 집값, 다리를, 이가탄, 프랑스판, 타면, 건조한, 맹수들앞에서,\n",
      "Nearest to 중: 순간포착, 용사는, 끝판대장, 특사로, 신인의, 강추위를, 말한, 메이비를,\n",
      "Nearest to 때: 물렀거라, 다른세상, 이하늬, 전북, 운용, 자동차가, 평가절하, 오픈했다고,\n",
      "Nearest to 무슨: 허리통증, 김소영, 닮아도, 대중이, 장담은, 웃음바다, 수영장에서, 유망주,\n",
      "Nearest to 다이어트: 회사를, 상경, 잠을, 게임개발, 스케줄은, 돌려서, 아이라면막막하네, 꽈리고추,\n",
      "Nearest to 첫: 베란다까지, 걸어봤니, 아빠만의, 행복한가, 드라기, 학력, 일명, 연예기자또,\n",
      "Nearest to 뉴스에이드: 향수는, 이순신, 도와주냐, 대자, 먹는다고, 파란별윤정, 죽어, 아이에요,\n",
      "Nearest to 만원: 마스크맨들, 꽃들의, 씨스타, 고기도, 확률은, 카이맨, 보이, 돌파구,\n",
      "Nearest to 하는: 아무, 쭉쭉, 서림, 첫째, 샌디스크코리아, 간담이, 산타의, 소송중인,\n",
      "Nearest to 전: 먼치킨, 예능꾼들, 일곱, 위생, 비슷한, 신기술, 표현하기, 산업의,\n"
     ]
    }
   ],
   "source": [
    "# Step 5: Begin training.\n",
    "num_steps = 100001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  # We must initialize all variables before we use them.\n",
    "  init.run()\n",
    "  print(\"Initialized\")\n",
    "\n",
    "  average_loss = 0\n",
    "  for step in xrange(num_steps):\n",
    "    batch_inputs, batch_labels = generate_batch(\n",
    "        batch_size, num_skips, skip_window)\n",
    "    feed_dict = {train_inputs: batch_inputs, train_labels: batch_labels}\n",
    "\n",
    "    # We perform one update step by evaluating the optimizer op (including it\n",
    "    # in the list of returned values for session.run()\n",
    "    _, loss_val = session.run([optimizer, loss], feed_dict=feed_dict)\n",
    "    average_loss += loss_val\n",
    "\n",
    "    if step % 2000 == 0:\n",
    "      if step > 0:\n",
    "        average_loss /= 2000\n",
    "      # The average loss is an estimate of the loss over the last 2000 batches.\n",
    "      print(\"Average loss at step \", step, \": \", average_loss)\n",
    "      average_loss = 0\n",
    "\n",
    "    # Note that this is expensive (~20% slowdown if computed every 500 steps)\n",
    "    if step % 10000 == 0:\n",
    "      sim = similarity.eval()\n",
    "      for i in xrange(valid_size):\n",
    "        valid_word = reverse_dictionary[valid_examples[i]]\n",
    "        top_k = 8  # number of nearest neighbors\n",
    "        nearest = (-sim[i, :]).argsort()[1:top_k + 1]\n",
    "        log_str = \"Nearest to %s:\" % valid_word\n",
    "        for k in xrange(top_k):\n",
    "          close_word = reverse_dictionary[nearest[k]]\n",
    "          log_str = \"%s %s,\" % (log_str, close_word)\n",
    "        print(log_str)\n",
    "  final_embeddings = normalized_embeddings.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n"
     ]
    }
   ],
   "source": [
    "print(len(sim[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 6: Visualize the embeddings.\n",
    "\n",
    "\n",
    "def plot_with_labels(low_dim_embs, labels, filename='tsne.png'):\n",
    "  assert low_dim_embs.shape[0] >= len(labels), \"More labels than embeddings\"\n",
    "  plt.figure(figsize=(18, 18))  # in inches\n",
    "  for i, label in enumerate(labels):\n",
    "    x, y = low_dim_embs[i, :]\n",
    "    plt.scatter(x, y)\n",
    "    plt.annotate(label,\n",
    "                 xy=(x, y),\n",
    "                 xytext=(5, 2),\n",
    "                 textcoords='offset points',\n",
    "                 ha='right',\n",
    "                 va='bottom')\n",
    "\n",
    "  plt.savefig(filename)\n",
    "\n",
    "try:\n",
    "  from sklearn.manifold import TSNE\n",
    "  import matplotlib.pyplot as plt\n",
    "\n",
    "  tsne = TSNE(perplexity=30, n_components=2, init='pca', n_iter=5000)\n",
    "  plot_only = 500\n",
    "  low_dim_embs = tsne.fit_transform(final_embeddings[:plot_only, :])\n",
    "  labels = [reverse_dictionary[i] for i in xrange(plot_only)]\n",
    "  plot_with_labels(low_dim_embs, labels)\n",
    "\n",
    "except ImportError:\n",
    "  print(\"Please install sklearn, matplotlib, and scipy to visualize embeddings.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
