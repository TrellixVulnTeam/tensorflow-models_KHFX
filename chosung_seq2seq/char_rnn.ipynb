{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#embedding 테스트 - 완료\n",
    "##one-hot으로 할 필요가 없음.\n",
    "##embedding_size도 마음대로 결정할 수 있음.\n",
    "##random init으로 해도됨.\n",
    "##input의 embedding을 training할수도 있고 안할수도 있음\n",
    "##Y는 embedding 할 필요 없음. 그냥 target class shape만 맞춰서 loss를 구하면 됨\n",
    "\n",
    "#file에서 읽어와서 input으로 넣어보기 - 완료\n",
    "##이미 index화 되어 있는 파일 읽어오기 - 완료\n",
    "##dictionary file로부터 dic 생성 - 완료\n",
    "##embedding 활용 - 완료\n",
    "\n",
    "#saver 적용 - 완료\n",
    "\n",
    "#tensorboard 적용\n",
    "\n",
    "#padding 적용\n",
    "\n",
    "#batch 적용\n",
    "\n",
    "#bidirectional rnn 적용\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pprint as pp\n",
    "import os\n",
    "import random\n",
    "\n",
    "from tensorflow.contrib import rnn\n",
    "tf.set_random_seed(777)  # reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "checkpoint_path = 'data/dev/ckpt_char/'\n",
    "\n",
    "def read_sentences_from_file(file_path):\n",
    "    sentence_file = open(file_path, 'r')\n",
    "    sentences = []\n",
    "    while True:\n",
    "        line = sentence_file.readline()\n",
    "        if not line:\n",
    "            break;\n",
    "        sentences.append(line.replace('\\n',''))\n",
    "    sentence_file.close()\n",
    "    return sentences\n",
    "\n",
    "def read_sentence_indexes_from_file(file_path):\n",
    "    sentence_index_file = open(file_path, 'r')\n",
    "    sentence_indexes = []\n",
    "    print(\"start reading \",file_path)\n",
    "    line_count = 0\n",
    "    while True:\n",
    "        line = sentence_index_file.readline()\n",
    "        if not line:\n",
    "            break;\n",
    "        sentence_indexes.append([int(c) for c in line.replace('\\n','').split()])\n",
    "        if line_count % 100000 == 0:\n",
    "            print(\"reading line \",line_count)\n",
    "        line_count+=1\n",
    "        \n",
    "    sentence_index_file.close()\n",
    "    return sentence_indexes\n",
    "\n",
    "def char_tokenizer(sentence): #sentence는 byte로 들어옴\n",
    "    sentence = sentence.replace('<english>','a').replace('<number>','0')\n",
    "    return [char for char in sentence] #그 후 최종 다시 byte로 변환한다\n",
    "\n",
    "def create_dic(sentences, save_file_path):\n",
    "    dic = []\n",
    "    file = open(save_file_path, 'w+')\n",
    "    for sentence in sentences:\n",
    "        tokens = char_tokenizer(sentence)\n",
    "        for token in tokens:\n",
    "            if not (token in dic):\n",
    "                dic.append(token)\n",
    "                file.write(token+'\\n')\n",
    "    file.close\n",
    "    return dic\n",
    "    \n",
    "def load_dic(dic_file_path):\n",
    "    dic_file = open(dic_file_path, 'r')\n",
    "    rev_vocab_dic = []\n",
    "    while True:\n",
    "        line = dic_file.readline()\n",
    "        if not line:\n",
    "            break;\n",
    "        rev_vocab_dic.append(line.replace('\\n',''))\n",
    "    vocab_dic = {w: i for i, w in enumerate(rev_vocab_dic)}\n",
    "    return vocab_dic, rev_vocab_dic\n",
    "\n",
    "def sentences_to_indexes(sentences, vocab_dic):\n",
    "    indexes = []\n",
    "    for sentence in sentences:\n",
    "        tokens = char_tokenizer(sentence)\n",
    "        indexes.append([vocab_dic[char] for char in tokens])\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start reading  data/train/namuwiki_sentences.txt.ids10000\n",
      "reading line  0\n",
      "reading line  100000\n",
      "reading line  200000\n",
      "reading line  300000\n",
      "reading line  400000\n",
      "reading line  500000\n",
      "reading line  600000\n",
      "reading line  700000\n",
      "reading line  800000\n",
      "reading line  900000\n"
     ]
    }
   ],
   "source": [
    "target_sentence_indexes =  read_sentence_indexes_from_file('data/train/namuwiki_sentences.txt.ids10000')\n",
    "target_vocab_dic, target_rev_vocab_dic = load_dic('data/dev/vocab_dev.to')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_size = 30\n",
    "rnn_depth = 2\n",
    "num_classes = len(target_rev_vocab_dic)\n",
    "embedding_size = 30\n",
    "sequence_length = 10 #window_size\n",
    "learning_rate = 0.1\n",
    "learning_rate_decay_rate = 0.99\n",
    "batch_sentence = 100\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_batch():\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    output_weights = []\n",
    "\n",
    "    for _ in range(batch_sentence):\n",
    "        i = random.randrange(0,len(target_sentence_indexes))\n",
    "        output_weight = np.ones(sequence_length, dtype=np.float32)\n",
    "        target_indexes = list(target_sentence_indexes[i])\n",
    "        if len(target_indexes) < sequence_length:\n",
    "            continue\n",
    "        \n",
    "        for cursor in range(0, len(target_indexes) - sequence_length):\n",
    "            x_indexes = target_indexes[cursor:cursor + sequence_length]\n",
    "            y_indexes = target_indexes[cursor + 1: cursor + sequence_length + 1]\n",
    "            dataX.append(x_indexes)\n",
    "            dataY.append(y_indexes)\n",
    "            output_weights.append(output_weight)\n",
    "        \n",
    "    return dataX[:batch_size], dataY[:batch_size], output_weights[:batch_size]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.int32, [None, sequence_length], name=\"input_characters\")\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length], name=\"target_characters\")\n",
    "weights = tf.placeholder(tf.float32, [None, sequence_length], name=\"sequence_weights\")\n",
    "learning_rate_var = tf.Variable(float(learning_rate), trainable=False, dtype=tf.float32)\n",
    "learning_rate_decay_op = learning_rate_var.assign(learning_rate_var * learning_rate_decay_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3548\n",
      "Tensor(\"one_hot:0\", shape=(?, 10, 3548), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(num_classes)\n",
    "# embeddings = tf.Variable(\n",
    "#     tf.random_uniform([len(target_vocab_dic), embedding_size], -1.0, 1.0), trainable=True, name=\"char_embeddings\")\n",
    "# embedded_input = tf.nn.embedding_lookup(embeddings, X, name=\"embedded_char_sentences\")\n",
    "embedded_input = tf.one_hot(X, num_classes)\n",
    "print(embedded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a lstm cell with hidden_size (each unit output vector size)\n",
    "\n",
    "def lstm_cell():\n",
    "    cell = rnn.BasicLSTMCell(hidden_size, state_is_tuple=True)\n",
    "    return cell\n",
    "\n",
    "cell = rnn.MultiRNNCell([lstm_cell() for _ in range(rnn_depth)], state_is_tuple=True)\n",
    "# cell = rnn.BasicLSTMCell(hidden_size, state_is_tuple=True)\n",
    "# cell = rnn.MultiRNNCell([cell] * rnn_depth, state_is_tuple=True)\n",
    "\n",
    "# outputs: unfolding size x hidden size, state = hidden size\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, embedded_input, dtype=tf.float32)\n",
    "\n",
    "# FC layer\n",
    "X_for_fc = tf.reshape(outputs, [-1, hidden_size], name=\"reshaped_rnn_output\")\n",
    "outputs = tf.contrib.layers.fully_connected(X_for_fc, num_classes, activation_fn=None)\n",
    "\n",
    "# reshape out for sequence_loss\n",
    "outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes], name=\"predictions\")\n",
    "\n",
    "force_weights = tf.ones([batch_size, sequence_length])\n",
    "\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(\n",
    "    logits=outputs, targets=Y, weights=force_weights, name=\"sequence_loss\")\n",
    "mean_loss = tf.reduce_mean(sequence_loss)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=learning_rate_var).minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model with fresh parameters.\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "saver = tf.train.Saver(tf.global_variables()) ##saver = tf.train.Saver({\"my_v2\": v2})\n",
    "\n",
    "ckpt = tf.train.get_checkpoint_state(checkpoint_path)\n",
    "if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "    print(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    print(\"Created model with fresh parameters.\")\n",
    "    sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "a1,b1,c1 = get_batch()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "256 256 256\n",
      "[42, 3, 15, 7, 3, 453, 330, 28, 18, 3] [3, 15, 7, 3, 453, 330, 28, 18, 3, 385] [ 1.  1.  1.  1.  1.  1.  1.  1.  1.  1.]\n"
     ]
    }
   ],
   "source": [
    "print(len(a1),len(b1),len(c1))\n",
    "ccc = 4\n",
    "print(a1[ccc],b1[ccc],c1[ccc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step: 0  loss: 0.0817420768738  learing_rate: 0.1\n",
      "뀔초봉탖줫땔땔땔값값봉맙맙쇀쇀똇똇똇똇십십십훡십쨌쨌쨌쎔쎔쨱---------------\n",
      "saved checkpoint\n",
      "step: 100  loss: 5.1945008564  learing_rate: 0.1\n",
      "이                             ---------------\n",
      "saved checkpoint\n",
      "step: 200  loss: 4.62480917454  learing_rate: 0.1\n",
      "은  aa   aa  a이 aa a이   a이    a---------------\n",
      "saved checkpoint\n",
      "step: 300  loss: 4.40335816383  learing_rate: 0.1\n",
      " a이 a이a이 a  a이a  a이  a이  a이   ---------------\n",
      "saved checkpoint\n",
      "step: 400  loss: 4.34262550831  learing_rate: 0.1\n",
      "a      a    a리 고 a   a  a    a---------------\n",
      "saved checkpoint\n",
      "step: 500  loss: 4.23564789772  learing_rate: 0.1\n",
      " 0 0   0리 기0 0이  리 0리 는 0는는   ---------------\n",
      "saved checkpoint\n"
     ]
    }
   ],
   "source": [
    "step_to_test = 100\n",
    "test_sentence_length = 20\n",
    "loss_history = []\n",
    "step_loss = 0.0\n",
    "test_results = []\n",
    "for i in range(10000000): ##for test\n",
    "    dataX, dataY, output_weights = get_batch()\n",
    "    _, l, results, label_y, learning_rate_check = sess.run(\n",
    "        [train_op, mean_loss, outputs, Y, learning_rate_var], feed_dict={X: dataX, Y: dataY, weights: output_weights})\n",
    "    step_loss += (l / step_to_test)\n",
    "    \n",
    "    if i % step_to_test == 0:\n",
    "        if len(loss_history) > 2 and step_loss > max(loss_history[-3:]):\n",
    "            sess.run(learning_rate_decay_op)\n",
    "\n",
    "        loss_history.append(step_loss)\n",
    "        print('step:',i,' loss:',loss_history[-1], ' learing_rate:',learning_rate_check)\n",
    "        \n",
    "        step_loss = 0.0\n",
    "    \n",
    "        for j, result in enumerate(results):\n",
    "            if j > test_sentence_length:\n",
    "                break\n",
    "            index = np.argmax(result, axis=1)\n",
    "            if j is 0:  # print all for the first result to make a sentence\n",
    "                print(''.join([target_rev_vocab_dic[t] for t in index]), end='')\n",
    "            else:\n",
    "                print(target_rev_vocab_dic[index[-1]], end='')\n",
    "\n",
    "        print('---------------')\n",
    "        checkpoint_file_path = os.path.join(checkpoint_path, \"char_rnn.ckpt\")\n",
    "        saver.save(sess, checkpoint_file_path)\n",
    "        print('saved checkpoint')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# np.argmax(test_results[0], axis=0)\n",
    "np.argmax(test_results[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(results)) #batch size\n",
    "print(len(results[0])) #sequence length\n",
    "print(len(results[0][0])) #vocab_size\n",
    "\n",
    "print(batch_size)\n",
    "print(sequence_length)\n",
    "print(len(target_vocab_dic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#embedding 연습!!\n",
    "import tensorflow as tf\n",
    "\n",
    "embeddings = tf.Variable(\n",
    "    tf.random_uniform([num_classes, 50], -1.0, 1.0))\n",
    "\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[3,2])\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "ems, inputs,embd = sess.run([embeddings,train_inputs,embed], feed_dict={train_inputs:[[1,2],[2,3],[3,4]]})\n",
    "\n",
    "for i in ems:\n",
    "    print(i)\n",
    "print('-----------')\n",
    "print(inputs)\n",
    "print('-----------')\n",
    "print(embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
