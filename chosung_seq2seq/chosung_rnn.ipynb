{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#embedding 테스트 - 완료\n",
    "##one-hot으로 할 필요가 없음.\n",
    "##embedding_size도 마음대로 결정할 수 있음.\n",
    "##random init으로 해도됨.\n",
    "##input의 embedding을 training할수도 있고 안할수도 있음\n",
    "##Y는 embedding 할 필요 없음. 그냥 target class shape만 맞춰서 loss를 구하면 됨\n",
    "\n",
    "#file에서 읽어와서 input으로 넣어보기 - 완료\n",
    "##이미 index화 되어 있는 파일 읽어오기 - 완료\n",
    "##dictionary file로부터 dic 생성 - 완료\n",
    "##embedding 활용 - 완료\n",
    "\n",
    "#saver 적용 - 완료\n",
    "\n",
    "#tensorboard 적용\n",
    "\n",
    "#padding 적용\n",
    "\n",
    "#batch 적용\n",
    "\n",
    "#bidirectional rnn 적용\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pprint as pp\n",
    "import os\n",
    "import random\n",
    "\n",
    "from tensorflow.contrib import rnn\n",
    "tf.set_random_seed(777)  # reproducibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "checkpoint_path = 'data/dev/ckpt/'\n",
    "\n",
    "def read_sentences_from_file(file_path):\n",
    "    sentence_file = open(file_path, 'r')\n",
    "    sentences = []\n",
    "    while True:\n",
    "        line = sentence_file.readline()\n",
    "        if not line:\n",
    "            break;\n",
    "        sentences.append(line.replace('\\n',''))\n",
    "    sentence_file.close()\n",
    "    return sentences\n",
    "\n",
    "def read_sentence_indexes_from_file(file_path):\n",
    "    sentence_index_file = open(file_path, 'r')\n",
    "    sentence_indexes = []\n",
    "    print(\"start reading \",file_path)\n",
    "    line_count = 0\n",
    "    while True:\n",
    "        line = sentence_index_file.readline()\n",
    "        if not line:\n",
    "            break;\n",
    "        sentence_indexes.append([int(c) for c in line.replace('\\n','').split()])\n",
    "        if line_count % 100000 == 0:\n",
    "            print(\"reading line \",line_count)\n",
    "        line_count+=1\n",
    "        \n",
    "    sentence_index_file.close()\n",
    "    return sentence_indexes\n",
    "\n",
    "def char_tokenizer(sentence): #sentence는 byte로 들어옴\n",
    "    sentence = sentence.replace('<english>','a').replace('<number>','0')\n",
    "    return [char for char in sentence] #그 후 최종 다시 byte로 변환한다\n",
    "\n",
    "def create_dic(sentences, save_file_path):\n",
    "    dic = []\n",
    "    file = open(save_file_path, 'w+')\n",
    "    for sentence in sentences:\n",
    "        tokens = char_tokenizer(sentence)\n",
    "        for token in tokens:\n",
    "            if not (token in dic):\n",
    "                dic.append(token)\n",
    "                file.write(token+'\\n')\n",
    "    file.close\n",
    "    return dic\n",
    "    \n",
    "def load_dic(dic_file_path):\n",
    "    dic_file = open(dic_file_path, 'r')\n",
    "    rev_vocab_dic = []\n",
    "    while True:\n",
    "        line = dic_file.readline()\n",
    "        if not line:\n",
    "            break;\n",
    "        rev_vocab_dic.append(line.replace('\\n',''))\n",
    "    vocab_dic = {w: i for i, w in enumerate(rev_vocab_dic)}\n",
    "    return vocab_dic, rev_vocab_dic\n",
    "\n",
    "def sentences_to_indexes(sentences, vocab_dic):\n",
    "    indexes = []\n",
    "    for sentence in sentences:\n",
    "        tokens = char_tokenizer(sentence)\n",
    "        indexes.append([vocab_dic[char] for char in tokens])\n",
    "    return indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##직접 dic을 구축하고, index를 만드려면 아래의 코드를 사용\n",
    "# chosung_sentences = read_sentences_from_file('data/dev/namuwiki_chosung_dev.txt')\n",
    "# target_sentences = read_sentences_from_file('data/dev/namuwiki_sentences_dev.txt')\n",
    "\n",
    "# _ = create_dic(chosung_sentences, 'data/dev/chosung_dic_dev')\n",
    "# _ = create_dic(target_sentences, 'data/dev/target_dic_dev')\n",
    "\n",
    "# chosung_vocab_dic, chosung_rev_vocab_dic = load_dic('data/dev/chosung_dic_dev')\n",
    "# target_vocab_dic, target_rev_vocab_dic = load_dic('data/dev/target_dic_dev')\n",
    "\n",
    "# chosung_sentence_indexes = sentences_to_indexes(chosung_sentences, chosung_vocab_dic)\n",
    "# target_sentence_indexes = sentences_to_indexes(target_sentences, target_vocab_dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start reading  data/train/namuwiki_chosung.txt.ids10000\n",
      "reading line  0\n",
      "reading line  100000\n",
      "reading line  200000\n",
      "reading line  300000\n",
      "reading line  400000\n",
      "reading line  500000\n",
      "reading line  600000\n",
      "reading line  700000\n",
      "reading line  800000\n",
      "reading line  900000\n",
      "start reading  data/train/namuwiki_sentences.txt.ids10000\n",
      "reading line  0\n",
      "reading line  100000\n",
      "reading line  200000\n",
      "reading line  300000\n",
      "reading line  400000\n",
      "reading line  500000\n",
      "reading line  600000\n",
      "reading line  700000\n",
      "reading line  800000\n",
      "reading line  900000\n"
     ]
    }
   ],
   "source": [
    "##이미 만들어진 dic과 index를 사용하려면 아래의 코드를 사용\n",
    "chosung_sentence_indexes =  read_sentence_indexes_from_file('data/train/namuwiki_chosung.txt.ids10000')\n",
    "target_sentence_indexes =  read_sentence_indexes_from_file('data/train/namuwiki_sentences.txt.ids10000')\n",
    "chosung_vocab_dic, chosung_rev_vocab_dic = load_dic('data/dev/vocab_dev.from')\n",
    "target_vocab_dic, target_rev_vocab_dic = load_dic('data/dev/vocab_dev.to')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "hidden_size = 30\n",
    "num_classes = len(target_rev_vocab_dic)\n",
    "embedding_size = 30\n",
    "sequence_length = 50\n",
    "learning_rate = 0.05\n",
    "batch_size = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_batch():\n",
    "    dataX = []\n",
    "    dataY = []\n",
    "    output_weights = []\n",
    "    sequence_length_arr = []\n",
    "\n",
    "    for _ in range(batch_size):\n",
    "        i = random.randrange(0,len(chosung_sentence_indexes))\n",
    "        output_weight = np.ones(sequence_length, dtype=np.float32)\n",
    "        chosung_indexes = list(chosung_sentence_indexes[i])\n",
    "        target_indexes = list(target_sentence_indexes[i])\n",
    "        if len(chosung_indexes) > sequence_length:\n",
    "            chosung_indexes = chosung_indexes[:sequence_length]\n",
    "            target_indexes = target_indexes[:sequence_length]\n",
    "            sequence_length_arr.append(sequence_length)\n",
    "        else:\n",
    "            sequence_length_arr.append(len(chosung_indexes))\n",
    "            pad_size = sequence_length - len(chosung_indexes)\n",
    "            chosung_indexes += [0]*pad_size #padding\n",
    "            target_indexes += [0]*pad_size #padding\n",
    "            for j in range(pad_size):\n",
    "                output_weight[sequence_length-1-j] = 0.0 #padding인 경우 weight를 0으로 초기화\n",
    "        dataX.append(chosung_indexes) \n",
    "        dataY.append(target_indexes)\n",
    "        output_weights.append(output_weight)\n",
    "        \n",
    "    return dataX, dataY, output_weights, sequence_length_arr\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = tf.placeholder(tf.int32, [None, sequence_length], name=\"chosung_sentences\")\n",
    "Y = tf.placeholder(tf.int32, [None, sequence_length], name=\"target_sentences\")\n",
    "weights = tf.placeholder(tf.float32, [None, sequence_length], name=\"seqence_weights\")\n",
    "output_sequence_size = tf.placeholder(tf.int32, [None], name=\"output_sequence_size\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3548\n",
      "Tensor(\"embedded_chosung_sentences:0\", shape=(?, 50, 30), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(num_classes)\n",
    "embeddings = tf.Variable(\n",
    "    tf.random_uniform([len(chosung_vocab_dic), embedding_size], -1.0, 1.0), trainable=False, name=\"chosung_embeddings\")\n",
    "embedded_input = tf.nn.embedding_lookup(embeddings, X, name=\"embedded_chosung_sentences\")\n",
    "print(embedded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Make a lstm cell with hidden_size (each unit output vector size)\n",
    "cell = rnn.BasicLSTMCell(hidden_size, state_is_tuple=True)\n",
    "cell = rnn.MultiRNNCell([cell] * 2, state_is_tuple=True)\n",
    "\n",
    "# outputs: unfolding size x hidden size, state = hidden size\n",
    "outputs, _states = tf.nn.dynamic_rnn(cell, embedded_input, dtype=tf.float32, sequence_length=output_sequence_size)\n",
    "\n",
    "# FC layer\n",
    "X_for_fc = tf.reshape(outputs, [-1, hidden_size], name=\"reshaped_rnn_output\")\n",
    "outputs = tf.contrib.layers.fully_connected(X_for_fc, num_classes, activation_fn=None)\n",
    "\n",
    "# reshape out for sequence_loss\n",
    "outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes], name=\"predictions\")\n",
    "\n",
    "sequence_loss = tf.contrib.seq2seq.sequence_loss(\n",
    "    logits=outputs, targets=Y, weights=weights, name=\"sequence_loss\")\n",
    "mean_loss = tf.reduce_mean(sequence_loss)\n",
    "train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(mean_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created model with fresh parameters.\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "saver = tf.train.Saver(tf.global_variables())\n",
    "\n",
    "ckpt = tf.train.get_checkpoint_state(checkpoint_path)\n",
    "if ckpt and tf.train.checkpoint_exists(ckpt.model_checkpoint_path):\n",
    "    print(\"Reading model parameters from %s\" % ckpt.model_checkpoint_path)\n",
    "    saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "else:\n",
    "    print(\"Created model with fresh parameters.\")\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(50000):\n",
    "    dataX, dataY, output_weights, sequence_length_arr = get_batch()\n",
    "    _, l, results = sess.run(\n",
    "        [train_op, mean_loss, outputs], feed_dict={X: dataX, Y: dataY, weights: output_weights, output_sequence_size: sequence_length_arr})\n",
    "    if (i+1) % 100 == 0:\n",
    "        print('step:',i,' loss:',l)\n",
    "        results = sess.run(outputs, feed_dict={X: dataX, output_sequence_size: sequence_length_arr})\n",
    "        for sentence_result in results:\n",
    "            index = np.argmax(sentence_result, axis=1)\n",
    "            print(''.join([target_rev_vocab_dic[t] for t in index]))\n",
    "        print('----------')\n",
    "        checkpoint_file_path = os.path.join(checkpoint_path, \"chosung_rnn.ckpt\")\n",
    "        saver.save(sess, checkpoint_file_path)\n",
    "        print('saved checkpoint')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(len(results)) #batch size\n",
    "print(len(results[0])) #sequence length\n",
    "print(len(results[0][0])) #vocab_size\n",
    "\n",
    "print(batch_size)\n",
    "print(sequence_length)\n",
    "print(len(target_vocab_dic))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#embedding 연습!!\n",
    "import tensorflow as tf\n",
    "\n",
    "embeddings = tf.Variable(\n",
    "    tf.random_uniform([num_classes, 50], -1.0, 1.0))\n",
    "\n",
    "train_inputs = tf.placeholder(tf.int32, shape=[3,2])\n",
    "embed = tf.nn.embedding_lookup(embeddings, train_inputs)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "ems, inputs,embd = sess.run([embeddings,train_inputs,embed], feed_dict={train_inputs:[[1,2],[2,3],[3,4]]})\n",
    "\n",
    "for i in ems:\n",
    "    print(i)\n",
    "print('-----------')\n",
    "print(inputs)\n",
    "print('-----------')\n",
    "print(embd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
